{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluate AI Agent on Azure**\n",
    "\n",
    "In this notebook, we focus on **evaluating the AI agent using Azure services**. This involves importing the required libraries, loading the necessary configurations, performing evaluations using Azure AI services, and analyzing the results.\n",
    "\n",
    "### Objectives:\n",
    "- **Import Libraries:** Import the necessary libraries for evaluation.\n",
    "- **Load Configurations:** Load the necessary configurations from the environment file.\n",
    "- **Perform Evaluation:** Use your LLM judge to evaluate the AI agent.\n",
    "- **Analyze Results:** Analyze the evaluation results to gain insights into the AI agent's performance.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Import Libraries:** Import the necessary libraries for evaluation.\n",
    "2. **Load Configurations:** Load the necessary configurations from the environment file.\n",
    "3. **Perform Evaluation:** Use your LLM jduge to evaluate the AI agent.\n",
    "4. **Analyze Results:** Analyze the evaluation results to gain insights into the AI agent's performance.\n",
    "\n",
    "This notebook ensures that the AI agent is evaluated effectively using your LLM judge, providing insights into its performance and areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_endpoint=os.environ[\"AZURE_OPENAI_API_BASE\"]\n",
    "aoai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "aoai_chat_model_mini=os.environ[\"AZURE_OPENAI_MODEL_MINI\"]\n",
    "llm_judge=os.environ[\"LLM_JUDGE\"]\n",
    "aoai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform data to correct format**\n",
    "- *For demonstration purposes, we use the testdata and transform it to the correct jsonl format and run the evaluation flow*\n",
    "- *However, it's intended to be run using data from your RAG system.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# This notebook is designed to evaluate data from your RAG solution and pass it through an evaluator.\n",
    "# The results can be viewed locally, or uploaded to Azure AI Foundry for better visibility.\n",
    "#\n",
    "# For demonstration purposes, we are using the test dataset to simulate this workflow.\n",
    "# You should replace the test dataset with your actual input/output data.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "fpath = 'data/ft-judge/single/test.csv'\n",
    "df = pd.read_csv(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Data Structure:\n",
    "#   The evaluation data should be stored in a JSON Lines (jsonl) file, where each line is a separate JSON object.\n",
    "# Each JSON object has the following keys:\n",
    "# - \"query\":   The query provided to the AI agent.\n",
    "# - \"context\": Additional context information that may include instructions, background details,\n",
    "#              or any relevant text that assists in generating the correct response.\n",
    "# - \"response\": The actual response generated by the AI agent for the given query.\n",
    "\n",
    "df_subset = df[['synthetic_question', 'chunk_data', 'synthetic_response']].rename(\n",
    "    columns={\n",
    "        'synthetic_question': 'query',\n",
    "        'chunk_data': 'context',\n",
    "        'synthetic_response': 'response'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Export the DataFrame to a JSONL file.\n",
    "eval_data_path = 'data/agent-output/testtest.jsonl'\n",
    "df_subset.to_json(eval_data_path, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run evaluation and upload results to AI Foundry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_config(eval_model=llm_judge):\n",
    "    return {\n",
    "        \"azure_endpoint\": aoai_endpoint,\n",
    "        \"api_key\": aoai_api_key,\n",
    "        \"azure_deployment\": eval_model,\n",
    "        \"api_version\": aoai_api_version\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(eval_model=llm_judge):\n",
    "    credential = DefaultAzureCredential()\n",
    "\n",
    "    model_config = get_model_config(eval_model)\n",
    "\n",
    "    # Initialize Azure AI project and Azure OpenAI conncetion with your environment variables\n",
    "    azure_ai_project = {\n",
    "        \"subscription_id\": os.environ[\"SUB_ID\"],\n",
    "        \"resource_group_name\": os.environ[\"RG_NAME\"],\n",
    "        \"project_name\": os.environ[\"AZURE_PROJECT_NAME\"],\n",
    "    }\n",
    "    return azure_ai_project, model_config, credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval_on_azure(azure_ai_project, custom_groundedness, model_name, path):\n",
    "    now = datetime.datetime.now()\n",
    "    result = evaluate(\n",
    "        evaluation_name = f\"custom-groundedness-{model_name}-{now.strftime('%Y-%m-%d-%H-%M-%S')}\",\n",
    "        data=path,\n",
    "        evaluators={\n",
    "            \"custom_groundedness_0_1\": custom_groundedness,\n",
    "        },\n",
    "        # column mapping\n",
    "        evaluator_config={\n",
    "            \"custom_groundedness\": {\n",
    "                \"column_mapping\": {\n",
    "                    \"query\": \"${data.query}\",\n",
    "                    \"context\": \"${data.context}\",\n",
    "                    \"response\": \"${data.response}\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        azure_ai_project = azure_ai_project,\n",
    "        # output_path=\"./myevalresults.json\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluators.aoai.custom_groundedness import CustomGroundednessEvaluator\n",
    "\n",
    "# Load Azure AI project and model configuration\n",
    "azure_ai_project, model_config, credential = load_config(eval_model=llm_judge)\n",
    "\n",
    "# Custom evaluator for groundedness\n",
    "custom_groundedness = CustomGroundednessEvaluator(model_config)\n",
    "\n",
    "# Run evaluation\n",
    "model_name = model_config[\"azure_deployment\"]\n",
    "run_eval_on_azure(azure_ai_project, custom_groundedness, model_name, eval_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "povel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
