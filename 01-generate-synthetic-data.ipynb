{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generate Synthetic Data**\n",
    "\n",
    "In this notebook, we focus on **generating synthetic data** for training and evaluation purposes. This involves retrieving chunks of data, generating synthetic questions based on these chunks, and filtering and embedding the generated questions.\n",
    "\n",
    "### Objectives:\n",
    "- **Retrieve Data Chunks:** Retrieve chunks of data from the search index.\n",
    "- **Generate Synthetic Questions:** Generate synthetic questions based on the retrieved chunks.\n",
    "- **Filter Questions:** Filter the generated questions to ensure quality.\n",
    "- **Embed Questions:** Embed the filtered questions for further use.\n",
    "- **Store Data:** Save the high-quality synthetic questions to a JSON Lines file.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Retrieve Data Chunks:** Retrieve chunks of data from the search index using Azure Cognitive Search.\n",
    "2. **Generate Synthetic Questions:** Use a synthetic data generator to create questions based on the retrieved chunks.\n",
    "3. **Filter Questions:** Filter the generated questions using criteria such as Jaccard similarity and embedding similarity.\n",
    "4. **Embed Questions:** Embed the filtered questions using a text embedding model.\n",
    "5. **Store Data:** Save the high-quality synthetic questions to a JSON Lines file for future use.\n",
    "\n",
    "This notebook ensures that high-quality synthetic data is generated, filtered, and stored effectively, providing valuable data for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values, load_dotenv\n",
    "from typing import List\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# load_dotenv(\".env\")\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "aoai_endpoint = config[\"AZURE_OPENAI_API_BASE\"]\n",
    "aoai_key = config[\"AZURE_OPENAI_API_KEY\"]\n",
    "aoai_api_version = config[\"AZURE_OPENAI_API_VERSION\"]\n",
    "aoai_chat_model = config[\"AZURE_OPENAI_MODEL\"]\n",
    "aoai_chat_model_mini = config[\"AZURE_OPENAI_MODEL_MINI\"]\n",
    "aoai_embedding_model = config[\"AZURE_OPENAI_EMBEDDING_MODEL\"]\n",
    "search_endpoint = config[\"SEARCH_ENDPOINT\"]\n",
    "search_key = config[\"SEARCH_KEY\"]\n",
    "credential = AzureKeyCredential(search_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Get all chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_chunks(index, k=1000):\n",
    "    try:\n",
    "        # perform search query\n",
    "        search_client = SearchClient(endpoint=search_endpoint, index_name=index, credential=credential)\n",
    "        results = search_client.search(\n",
    "            search_text=\"*\",\n",
    "            top=k # top parameter is capped at 1000 chunks/documents per retrieval call. If you have more than 1000 chunks/documents in your index, you will need to paginate through the results.\n",
    "        )\n",
    "\n",
    "        # format search results\n",
    "        data = []\n",
    "        for result in results:\n",
    "            data.append(\n",
    "                {\n",
    "                    \"chunk_id\": result[\"chunk_id\"],\n",
    "                    \"title\": result[\"title\"],\n",
    "                    \"chunk\": result[\"chunk\"],\n",
    "                    \"chunk_embedding\": result[\"text_vector\"]\n",
    "                }\n",
    "            )\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generate Synthetic chunk-dependent questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\povelf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.genai.generate_synthetic_data import GenerateSyntheticData, generate_synthetic_questions\n",
    "from utils.genai.filter_synthetic_data import filter_synthetic_questions\n",
    "from utils.genai.invoker import embed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_config():\n",
    "    dotenv.load_dotenv(\".env\")\n",
    "    return {\n",
    "        \"azure_endpoint\": aoai_endpoint,\n",
    "        \"api_key\": aoai_key,\n",
    "        \"azure_deployment\": aoai_chat_model, #\"gpt-4o-mini-sdg-llm\"\n",
    "        \"api_version\": aoai_api_version\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieve chunks from index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"product-customer-vector\"\n",
    "\n",
    "chunks = get_all_chunks(\n",
    "    index=index_name,\n",
    "    k=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate synthetic samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating synthetic questions: 100%|██████████| 30/30 [03:52<00:00,  7.76s/it]\n"
     ]
    }
   ],
   "source": [
    "# TODO: multi-chunk generation is not currently supported\n",
    "multi_choice=False\n",
    "\n",
    "# Get the model configuration\n",
    "model_config = get_model_config()\n",
    "\n",
    "# Instantiate your generator\n",
    "sdg_generator = GenerateSyntheticData(model_config, multi=multi_choice)\n",
    "\n",
    "# Generate synthetic chunk-dependent questions, each repeated N times with varied parameters\n",
    "synthetic_data, failed_samples = generate_synthetic_questions(chunks, sdg_generator, num_questions_per_chunk=1, multi=multi_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'synthetic_question': 'What was the total amount spent by David Kim on backpacks in the provided purchase records?',\n",
       " 'explanation': 'The question is grounded as it directly asks for the total amount spent on backpacks, which can be calculated using the purchase records provided in the chunk. The response accurately calculates the total cost by summing the cost of Adventurer Pro Backpacks and SummitClimber Backpacks, aligning with the information in the chunk.',\n",
       " 'synthetic_response': 'David Kim spent a total of $840 on backpacks. He purchased two Adventurer Pro Backpacks for $180 each, totaling $360, and two SummitClimber Backpacks for $240 each, totaling $480. The combined total for the backpacks is $840.',\n",
       " 'chunk_id': '252167c94bb6_aHR0cHM6Ly9zdG9yYWdlcG92ZWwuYmxvYi5jb3JlLndpbmRvd3MubmV0L2NvbnRhaW5lcnBvdmVsL2N1c3RvbWVyXzUubWQ1_pages_0_synthetic_1',\n",
       " 'is_grounded': True,\n",
       " 'chunk_data': '## Customer_Info\\n\\nFirst Name: David \\nLast Name: Kim \\nAge: 42 \\nEmail Address: davidkim@example.com \\nPhone Number: 555-555-5555 \\nShipping Address: 654 Pine St,  Suburbia USA, 23456 \\nMembership: Gold \\n\\n## Recent_Purchases\\n\\norder_number: 7 \\ndate: 2023-02-15 \\nitem:\\n- description:  Adventurer Pro Backpack, quantity 2, price $180 \\n\\xa0 item_number: 2 \\n\\norder_number: 16 \\ndate: 2023-02-25 \\nitem:\\n- description:  TrekReady Hiking Boots, quantity 2, price $280 \\n\\xa0 item_number: 4 \\n\\norder_number: 24 \\ndate: 2023-03-05 \\nitem:\\n- description:  EcoFire Camping Stove, quantity 2, price $160 \\n\\xa0 item_number: 6 \\n\\norder_number: 33 \\ndate: 2023-03-20 \\nitem:\\n- description:  SummitClimber Backpack, quantity 2, price $240 \\n\\xa0 item_number: 9 \\n\\norder_number: 45 \\ndate: 2023-04-11 \\nitem:\\n- description:  PowerBurner Camping Stove, quantity 2, price $200 \\n\\xa0 item_number: 13 \\n\\norder_number: 54 \\ndate: 2023-04-26 \\nitem:\\n- description:  TrailLite Daypack, quantity 2, price $120 \\n\\xa0 item_number: 16 \\n\\norder_number: 63 \\ndate: 2023-05-11 \\nitem:\\n- description:  Adventure Dining Table, quantity 2, price $180 \\n\\xa0 item_number: 19',\n",
       " 'aggregated_context': 'single chunk was used',\n",
       " 'question_number': 0,\n",
       " 'domain': 'Usage instructions',\n",
       " 'difficulty': 'Intermediate',\n",
       " 'tone': 'Friendly',\n",
       " 'language': 'English',\n",
       " 'question_length': 17}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter out low quality samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering synthetic questions: 100%|██████████| 30/30 [00:00<00:00, 4117.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of filtered out synthetic questions: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accepted_samples, rejected_samples = filter_synthetic_questions(\n",
    "    synthetic_data=synthetic_data,\n",
    "    jaccard_threshold=0.8,\n",
    "    embedding_threshold=0.8,\n",
    "    combination_mode=\"or\",\n",
    "    min_question_length=5,\n",
    "    max_question_length=150,\n",
    "    use_embedding=False\n",
    ")\n",
    "\n",
    "print(f\"\\nNumber of filtered out synthetic questions: {len(rejected_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding all questions: 100%|██████████| 30/30 [00:29<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Count total questions\n",
    "total_questions = len(accepted_samples)\n",
    "\n",
    "# Create a single progress bar\n",
    "with tqdm(total=total_questions, desc=\"Embedding all questions\") as pbar:\n",
    "    for item in accepted_samples:\n",
    "        item[\"synthetic_embedding\"] = embed_text(item[\"synthetic_question\"])\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Storing synthetic data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(json_list, file_path):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # Open the file in append mode so existing data is preserved\n",
    "    with open(file_path, \"w\") as f:\n",
    "        for obj in json_list:\n",
    "            f.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(accepted_samples, \"data/ft-judge/single/chunk-specific-synthetic-questions.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "povel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
