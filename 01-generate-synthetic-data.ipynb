{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generate Synthetic Data**\n",
    "\n",
    "In this notebook, we focus on **generating synthetic data** for training and evaluation purposes. This involves retrieving chunks of data, generating synthetic questions based on these chunks, and filtering and embedding the generated questions.\n",
    "\n",
    "### Objectives:\n",
    "- **Retrieve Data Chunks:** Retrieve chunks of data from the search index.\n",
    "- **Generate Synthetic Questions:** Generate synthetic questions based on the retrieved chunks.\n",
    "- **Filter Questions:** Filter the generated questions to ensure quality.\n",
    "- **Embed Questions:** Embed the filtered questions for further use.\n",
    "- **Store Data:** Save the high-quality synthetic questions to a JSON Lines file.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Retrieve Data Chunks:** Retrieve chunks of data from the search index using Azure Cognitive Search.\n",
    "2. **Generate Synthetic Questions:** Use a synthetic data generator to create questions based on the retrieved chunks.\n",
    "3. **Filter Questions:** Filter the generated questions using criteria such as Jaccard similarity and embedding similarity.\n",
    "4. **Embed Questions:** Embed the filtered questions using a text embedding model.\n",
    "5. **Store Data:** Save the high-quality synthetic questions to a JSON Lines file for future use.\n",
    "\n",
    "This notebook ensures that high-quality synthetic data is generated, filtered, and stored effectively, providing valuable data for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values, load_dotenv\n",
    "from typing import List\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# load_dotenv(\".env\")\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "aoai_endpoint = config[\"AZURE_OPENAI_API_BASE\"]\n",
    "aoai_key = config[\"AZURE_OPENAI_API_KEY\"]\n",
    "aoai_api_version = config[\"AZURE_OPENAI_API_VERSION\"]\n",
    "aoai_chat_model = config[\"AZURE_OPENAI_MODEL\"]\n",
    "aoai_chat_model_mini = config[\"AZURE_OPENAI_MODEL_MINI\"]\n",
    "aoai_embedding_model = config[\"AZURE_OPENAI_EMBEDDING_MODEL\"]\n",
    "search_endpoint = config[\"SEARCH_ENDPOINT\"]\n",
    "search_key = config[\"SEARCH_KEY\"]\n",
    "credential = AzureKeyCredential(search_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Get all chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_chunks(index, k=1000):\n",
    "    try:\n",
    "        # perform search query\n",
    "        search_client = SearchClient(endpoint=search_endpoint, index_name=index, credential=credential)\n",
    "        results = search_client.search(\n",
    "            search_text=\"*\",\n",
    "            top=k # top parameter is capped at 1000 chunks/documents per retrieval call. If you have more than 1000 chunks/documents in your index, you will need to paginate through the results.\n",
    "        )\n",
    "\n",
    "        # format search results\n",
    "        data = []\n",
    "        for result in results:\n",
    "            data.append(\n",
    "                {\n",
    "                    \"chunk_id\": result[\"chunk_id\"],\n",
    "                    \"title\": result[\"title\"],\n",
    "                    \"chunk\": result[\"chunk\"],\n",
    "                    \"chunk_embedding\": result[\"text_vector\"]\n",
    "                }\n",
    "            )\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = \"product-customer-vector\"\n",
    "\n",
    "chunks = get_all_chunks(\n",
    "    index=index_name,\n",
    "    k=30\n",
    ")\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generate Synthetic chunk-dependent questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\povelf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "import dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from utils.genai.generate_synthetic_questions import GenerateSyntheticData\n",
    "from utils.genai.filter_synthetic_questions import filter_synthetic_questions\n",
    "from utils.genai.invoker import embed_text\n",
    "from utils.sdg_generator_helper import (\n",
    "    get_instruction,\n",
    "    get_domain,\n",
    "    get_tone,\n",
    "    get_question_length,\n",
    "    get_difficulty,\n",
    "    get_topic,\n",
    "    get_language,\n",
    "    set_is_grounded\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_config():\n",
    "    dotenv.load_dotenv(\".env\")\n",
    "    return {\n",
    "        \"azure_endpoint\": aoai_endpoint,\n",
    "        \"api_key\": aoai_key,\n",
    "        \"azure_deployment\": aoai_chat_model, #\"gpt-4o-mini-sdg-llm\"\n",
    "        \"api_version\": aoai_api_version\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_similar_chunks(chunks, current_index, k=3):\n",
    "    \"\"\"\n",
    "    For the chunk at current_index, compute cosine similarity to all other chunks,\n",
    "    and return the texts of the top-k most similar chunks (excluding the current one).\n",
    "    \"\"\"\n",
    "    embeddings = np.array([chunk[\"chunk_embedding\"] for chunk in chunks])\n",
    "    current_embedding = embeddings[current_index].reshape(1, -1)\n",
    "    similarities = cosine_similarity(current_embedding, embeddings).flatten()\n",
    "    \n",
    "    similarities[current_index] = -np.inf # Exclude the current chunk by setting its similarity to -inf\n",
    "\n",
    "    top_k_indices = np.argpartition(similarities, -k)[-k:] # Get the indices of the top-k most similar chunks\n",
    "    top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])[::-1]] # Sort the indices by similarity\n",
    "    return [chunks[i][\"chunk\"] for i in top_k_indices]\n",
    "\n",
    "def build_multi_chunk_context(primary_chunk, additional_chunks):\n",
    "    \"\"\"\n",
    "    Combine the primary chunk with additional similar chunks into one context string.\n",
    "    Use clear delimiters to indicate different parts.\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    context_parts.append(\"Primary Chunk:\\n\" + primary_chunk)\n",
    "    for i, add_chunk in enumerate(additional_chunks, start=1):\n",
    "        context_parts.append(f\"Additional Context {i}:\\n\" + add_chunk)\n",
    "    \n",
    "    full_context = \"\\n\\n\".join(context_parts)\n",
    "    return full_context\n",
    "\n",
    "\n",
    "def generate_synthetic_questions(chunks, generator, num_questions_per_chunk, multi=False):\n",
    "    \"\"\"\n",
    "    For each chunk, retrieve the top_k similar chunks using cosine similarity,\n",
    "    build a multi-chunk context, and generate synthetic questions.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    failed_results = []\n",
    "    total_iterations = len(chunks) * num_questions_per_chunk\n",
    "\n",
    "    with tqdm(total=total_iterations, desc=\"Generating synthetic questions\") as pbar:\n",
    "        for current_index, chunk in enumerate(chunks):\n",
    "            if multi:\n",
    "                top_k=3\n",
    "                similar_chunks = get_top_k_similar_chunks(chunks, current_index, k=top_k)\n",
    "                multi_chunk_context = build_multi_chunk_context(chunk[\"chunk\"], similar_chunks)\n",
    "                context=multi_chunk_context\n",
    "                task_path = \"configs/settings/tasks/task_multi_grounded_not_grounded_questions.txt\"\n",
    "            else:\n",
    "                context = chunk[\"chunk\"]\n",
    "                task_path = \"configs/settings/tasks/task_single_grounded_not_grounded_questions.txt\"\n",
    "                multi_chunk_context = \"single chunk was used\"\n",
    "\n",
    "            for question_index in range(num_questions_per_chunk):\n",
    "                # Sample metadata for each question\n",
    "                domain = get_domain()\n",
    "                tone = get_tone()\n",
    "                difficulty = get_difficulty()\n",
    "                question_length = get_question_length(min_length=4)\n",
    "                topic = get_topic()\n",
    "                language = get_language()\n",
    "                instructions = \"None\"\n",
    "                task = get_instruction(task_path)\n",
    "                is_grounded = set_is_grounded()\n",
    "\n",
    "                try:\n",
    "                    # Call the generator with the multi-chunk context\n",
    "                    result = generator(\n",
    "                        chunk=context,\n",
    "                        is_grounded=is_grounded,\n",
    "                        domain=domain,\n",
    "                        difficulty=difficulty,\n",
    "                        topic=topic,\n",
    "                        language=language,\n",
    "                        instructions=instructions,\n",
    "                        question_length=question_length,\n",
    "                        task=task\n",
    "                    )\n",
    "                    # Since we requested one question, grab the first element.\n",
    "                    question = result[\"question\"][0]\n",
    "                    explanation = result[\"explanation\"][0]\n",
    "                    response = result[\"response\"][0]\n",
    "                    synthetic_chunk_id = f\"{chunk['chunk_id']}_synthetic_{question_index+1}\"\n",
    "\n",
    "\n",
    "                    all_results.append({\n",
    "                        \"synthetic_question\": question,\n",
    "                        \"explanation\": explanation,\n",
    "                        \"synthetic_response\": response,\n",
    "                        \"chunk_id\": synthetic_chunk_id,\n",
    "                        \"is_grounded\": is_grounded,\n",
    "                        \"chunk_data\": chunk[\"chunk\"],\n",
    "                        \"aggregated_context\": multi_chunk_context,\n",
    "                        \"question_number\": question_index,\n",
    "                        \"domain\": domain,\n",
    "                        \"difficulty\": difficulty,\n",
    "                        \"tone\": tone,\n",
    "                        \"language\": language,\n",
    "                        \"question_length\": question_length\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    failed_results.append({\n",
    "                        \"error\": str(e),\n",
    "                        \"chunk_id\": chunk[\"chunk_id\"],\n",
    "                        \"is_grounded\": is_grounded,\n",
    "                        \"chunk_data\": chunk[\"chunk\"],\n",
    "                        \"question_number\": question_index,\n",
    "                        \"domain\": domain,\n",
    "                        \"difficulty\": difficulty,\n",
    "                        \"tone\": tone,\n",
    "                        \"language\": language,\n",
    "                        \"question_length\": question_length\n",
    "                    })\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    return all_results, failed_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating synthetic questions: 100%|██████████| 30/30 [01:36<00:00,  3.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# TODO: multi-chunk generation is not currently supported\n",
    "multi_choice=False\n",
    "\n",
    "# Get the model configuration\n",
    "model_config = get_model_config()\n",
    "\n",
    "# Instantiate your generator\n",
    "sdg_generator = GenerateSyntheticData(model_config, multi=multi_choice)\n",
    "\n",
    "# Generate synthetic chunk-dependent questions, each repeated N times with varied parameters\n",
    "synthetic_data, failed_samples = generate_synthetic_questions(chunks, sdg_generator, num_questions_per_chunk=1, multi=multi_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'synthetic_question': 'What is the price of the SummitClimber Backpack purchased by David Kim?',\n",
       " 'explanation': \"The provided chunk contains detailed information about David Kim's recent purchases, including the SummitClimber Backpack. The description specifies the price of the SummitClimber Backpack as $240, making the question directly answerable from the chunk.\",\n",
       " 'synthetic_response': 'The SummitClimber Backpack purchased by David Kim costs $240.',\n",
       " 'chunk_id': '252167c94bb6_aHR0cHM6Ly9zdG9yYWdlcG92ZWwuYmxvYi5jb3JlLndpbmRvd3MubmV0L2NvbnRhaW5lcnBvdmVsL2N1c3RvbWVyXzUubWQ1_pages_0_synthetic_1',\n",
       " 'is_grounded': True,\n",
       " 'chunk_data': '## Customer_Info\\n\\nFirst Name: David \\nLast Name: Kim \\nAge: 42 \\nEmail Address: davidkim@example.com \\nPhone Number: 555-555-5555 \\nShipping Address: 654 Pine St,  Suburbia USA, 23456 \\nMembership: Gold \\n\\n## Recent_Purchases\\n\\norder_number: 7 \\ndate: 2023-02-15 \\nitem:\\n- description:  Adventurer Pro Backpack, quantity 2, price $180 \\n\\xa0 item_number: 2 \\n\\norder_number: 16 \\ndate: 2023-02-25 \\nitem:\\n- description:  TrekReady Hiking Boots, quantity 2, price $280 \\n\\xa0 item_number: 4 \\n\\norder_number: 24 \\ndate: 2023-03-05 \\nitem:\\n- description:  EcoFire Camping Stove, quantity 2, price $160 \\n\\xa0 item_number: 6 \\n\\norder_number: 33 \\ndate: 2023-03-20 \\nitem:\\n- description:  SummitClimber Backpack, quantity 2, price $240 \\n\\xa0 item_number: 9 \\n\\norder_number: 45 \\ndate: 2023-04-11 \\nitem:\\n- description:  PowerBurner Camping Stove, quantity 2, price $200 \\n\\xa0 item_number: 13 \\n\\norder_number: 54 \\ndate: 2023-04-26 \\nitem:\\n- description:  TrailLite Daypack, quantity 2, price $120 \\n\\xa0 item_number: 16 \\n\\norder_number: 63 \\ndate: 2023-05-11 \\nitem:\\n- description:  Adventure Dining Table, quantity 2, price $180 \\n\\xa0 item_number: 19',\n",
       " 'aggregated_context': 'single chunk was used',\n",
       " 'question_number': 0,\n",
       " 'domain': 'Usage instructions',\n",
       " 'difficulty': 'Beginner',\n",
       " 'tone': 'Neutral',\n",
       " 'language': 'English',\n",
       " 'question_length': 14}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering synthetic questions: 100%|██████████| 30/30 [00:00<00:00, 1997.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of filtered out synthetic questions: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accepted_samples, rejected_samples = filter_synthetic_questions(\n",
    "    synthetic_data=synthetic_data,\n",
    "    jaccard_threshold=0.8,\n",
    "    embedding_threshold=0.8,\n",
    "    combination_mode=\"or\",\n",
    "    min_question_length=5,\n",
    "    max_question_length=150,\n",
    "    use_embedding=False\n",
    ")\n",
    "\n",
    "print(f\"\\nNumber of filtered out synthetic questions: {len(rejected_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding all questions: 100%|██████████| 30/30 [00:04<00:00,  6.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# Count total questions\n",
    "total_questions = len(accepted_samples)\n",
    "\n",
    "# Create a single progress bar\n",
    "with tqdm(total=total_questions, desc=\"Embedding all questions\") as pbar:\n",
    "    for item in accepted_samples:\n",
    "        item[\"synthetic_embedding\"] = embed_text(item[\"synthetic_question\"])\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Storing synthetic data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_high_quality_synthetic_questions(json_list, path):\n",
    "    # Write the list of JSON objects to a JSON Lines file\n",
    "    with open(path, \"w\") as f:\n",
    "        for obj in json_list:\n",
    "            f.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_high_quality_synthetic_questions(accepted_samples, \"data/ft-judge/single/chunk-specific-synthetic-questions.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandvik-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
